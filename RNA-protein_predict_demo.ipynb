{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A demo for RNA-based protein expression prediction\n",
    "total running time: ~15 min ( the model named \"Stacking\" take most of time )\n",
    "Download X.tsv and Y.tsv from github repo to your local path, modify the path. You will have training data:\n",
    "1. RNA: 200 genes x 318 samples  \n",
    "2. Protein:100 genes x 318 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# Python [conda env:py3.7.3_skorch]\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import time, os, sys\n",
    "\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "\n",
    "import pickle\n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "nCPU= 5\n",
    "N=5\n",
    "np.random.seed(2021)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is how I cut out the demo data from  dataset 'liver_318_2019_tmt'\n",
    "if False:\n",
    "    ## prepre datasets(small)\n",
    "    ## full pkl available at https://www.ebi.ac.uk/biostudies/studies/S-BSST733\n",
    "    file = '/media/eys/xwj/proteome/data/20210926_dict_matrix_20dataset.pkl'\n",
    "    with open(file, 'rb') as f:\n",
    "        [ dict_dataset, df_summary]=pickle.load(f)\n",
    "        \n",
    "    mykey = 'liver_318_2019_tmt'\n",
    "    X = dict_dataset[mykey]['RNA'].iloc[:200, ]\n",
    "    Y = dict_dataset[mykey]['protein'].iloc[:100, ]\n",
    "    print(X.shape, Y.shape)\n",
    "    file = '/media/eys/xwj/proteome/data/liver_318_2019_tmt_X.tsv'\n",
    "    X.to_csv(file, sep='\\t')\n",
    "    file = '/media/eys/xwj/proteome/data/liver_318_2019_tmt_Y.tsv'\n",
    "    Y.to_csv(file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 318), (100, 318))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mykey = 'liver_318_2019_tmt'\n",
    "X=pd.read_csv('/media/eys/xwj/proteome/data/liver_318_2019_tmt_X.tsv', sep='\\t', index_col=0)\n",
    "Y=pd.read_csv('/media/eys/xwj/proteome/data/liver_318_2019_tmt_Y.tsv', sep='\\t', index_col=0)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### def models, settings, and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LR': LinearRegression(), 'Lasso': Lasso(alpha=0.02, max_iter=100000.0), 'HR': HuberRegressor(), 'Ridge': Ridge(), 'SVR': SVR(), 'RFR': RandomForestRegressor(max_depth=3, random_state=0), 'NN1': None, 'NN2': None, 'NN3': None, 'Stacking': StackingRegressor(estimators=[('LR', LinearRegression()),\n",
      "                              ('Lasso', Lasso(alpha=0.02, max_iter=100000.0)),\n",
      "                              ('HR', HuberRegressor()), ('Ridge', Ridge()),\n",
      "                              ('SVR', SVR()),\n",
      "                              ('RFR',\n",
      "                               RandomForestRegressor(max_depth=3,\n",
      "                                                     random_state=0))]), 'Voting': VotingRegressor(estimators=[('LR', LinearRegression()),\n",
      "                            ('Lasso', Lasso(alpha=0.02, max_iter=100000.0)),\n",
      "                            ('HR', HuberRegressor()), ('Ridge', Ridge()),\n",
      "                            ('SVR', SVR()),\n",
      "                            ('RFR',\n",
      "                             RandomForestRegressor(max_depth=3,\n",
      "                                                   random_state=0))]), 'Boosting': AdaBoostRegressor(random_state=0), 'Bagging': BaggingRegressor(random_state=0), 'baselineEN': ElasticNet(precompute=True, random_state=0), 'teamHYU': RandomForestRegressor(random_state=0), 'teamHL&YG': None} ['LR', 'Lasso', 'HR', 'Ridge', 'SVR', 'RFR', 'NN1', 'NN2', 'NN3', 'Stacking', 'Voting', 'Boosting', 'Bagging', 'baselineEN', 'teamHYU', 'teamHL&YG']\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    \"LR\": linear_model.LinearRegression(), \n",
    "    \"Lasso\": linear_model.Lasso(alpha=0.02, max_iter=1e5), \n",
    "    \"HR\": linear_model.HuberRegressor(), #Linear regression model that is robust to outliers.\n",
    "    \"Ridge\": linear_model.Ridge(), \n",
    "    \"SVR\": SVR( gamma='scale'),\n",
    "    \"RFR\": RandomForestRegressor(n_estimators=100, max_depth=3, random_state=0), \n",
    "    # NN : construct later, because feature length have to be set \n",
    "    \"NN1\": None,\n",
    "    \"NN2\": None,\n",
    "    \"NN3\": None,\n",
    "    ##### other ensemble models\n",
    "    \"Stacking\": None,\n",
    "    \"Voting\": None,\n",
    "    \"Boosting\": None,\n",
    "    \"Bagging\": None, \n",
    "    ##### reimplement published methods\n",
    "    \"baselineEN\": linear_model.ElasticNet(l1_ratio = 0.5, random_state = 0, precompute=True), \n",
    "    \"teamHYU\": RandomForestRegressor(n_estimators=100, random_state=0), # Author not providing detail, use default\n",
    "    \"teamHL&YG\": None, ## equivalent to RFR without feature selection\n",
    "    }\n",
    "\n",
    "basic_estimators = [\n",
    "    (\"LR\",  model_dict['LR']), \n",
    "    (\"Lasso\", model_dict['Lasso']), \n",
    "    (\"HR\",  model_dict['HR']),\n",
    "    (\"Ridge\", model_dict['Ridge']), \n",
    "    (\"SVR\", model_dict['SVR']),\n",
    "    (\"RFR\", model_dict['RFR']), \n",
    "    ]\n",
    " \n",
    "model_dict['Stacking'] = StackingRegressor(estimators=basic_estimators)\n",
    "model_dict['Voting'] = VotingRegressor(basic_estimators)\n",
    "model_dict['Boosting']  = AdaBoostRegressor(random_state=0)\n",
    "model_dict['Bagging'] = BaggingRegressor(random_state=0)\n",
    "    \n",
    "list_model = list(model_dict.keys())\n",
    "print( model_dict, list_model )\n",
    "method_feature_select = [\"cosine\", \"raw_cosine\", \"spearmanr\", \"random\", \"custom\"]\n",
    "\n",
    "# feature numbers to be prioritized in feature selection \n",
    "list_topn = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 200, 1000, 5000 ] # v1\n",
    "list_topn = [5, 10, 20, 30, 40, 50, 100, 200, 500, 1000, 5000 ] # v2\n",
    "list_topn = [10, 20  ] # v3\n",
    "\n",
    "#############\n",
    "def create_res(list_topn):\n",
    "    res = {}\n",
    "    for mfs in method_feature_select:\n",
    "        res[mfs] = {} \n",
    "        for topn in list_topn:\n",
    "            # summary table\n",
    "            res[mfs][topn] = {}\n",
    "            res[mfs][topn][\"time\"] = pd.DataFrame(data = 0, dtype = np.int32, index = model_dict, columns= [\"val\",\"train\",\"test\"])\n",
    "            # detail data for all models\n",
    "            for m in model_dict:\n",
    "                res[mfs][topn][m]={}\n",
    "                res[mfs][topn][m][\"y_pred_CVtrain\"], res[mfs][topn][m][\"y_pred_CVtest\"] ={}, {}\n",
    "    return res\n",
    "\n",
    "def comp_y_pred(p): ### this function fit a classical machine learning model and pred y value\n",
    "    # usage: comp_y_pred(\"ENSG00000000419\") \n",
    "    select=X_train.columns[ df_topn[p] ]\n",
    "    X_val_select, X_train_select, y_val, y_train = X_val[select], X_train[select], Y_val[p], Y_train[p]\n",
    "    my_model=model_dict[m].fit(X_train_select, y_train) #'''fit model with selected features'''\n",
    "    # training fit and test fit\n",
    "    return my_model.predict(X_train_select), my_model.predict(X_val_select)\n",
    "    \n",
    "class RegressorModule3(nn.Module):## 10, 10, 10\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "            n_feature=10,\n",
    "    ):\n",
    "        super(RegressorModule3, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(n_feature,  num_units)\n",
    "        self.dense1 = nn.Linear(num_units, num_units)\n",
    "        self.dense2 = nn.Linear(num_units, num_units)\n",
    "        self.output = nn.Linear(num_units, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.nonlin(self.dense2(X))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "    \n",
    "class RegressorModule2(nn.Module):## 50, 10\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "            n_feature=10,\n",
    "    ):\n",
    "        super(RegressorModule2, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(n_feature,  num_units*5)\n",
    "        self.dense1 = nn.Linear(num_units*5, num_units)\n",
    "        self.output = nn.Linear(num_units, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "class RegressorModule1(nn.Module): ## 10, 10\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "            n_feature=10,\n",
    "    ):\n",
    "        super(RegressorModule1, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(n_feature,  num_units)\n",
    "        self.dense1 = nn.Linear(num_units, num_units)\n",
    "        self.output = nn.Linear(num_units, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = self.nonlin(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "def comp_y_pred_nn3(p): ### this function fit a deep Neural network model and pred y value\n",
    "    select=X_train.columns[ df_topn[p] ]\n",
    "    X_val_select, X_train_select, y_val, y_train = X_val[select], X_train[select], Y_val[p], Y_train[p]\n",
    "    #'''fit model with selected features'''\n",
    "    my_model = NeuralNetRegressor( RegressorModule3(n_feature = topn ), max_epochs=10, lr=0.01, verbose=0, train_split=False)\n",
    "    \n",
    "    my_model.fit(X_train_select.values.astype(np.float32), y_train.values.astype(np.float32).reshape(-1,1))\n",
    "    return np.nan_to_num( my_model.predict(X_train_select.values.astype(np.float32))),                 np.nan_to_num( my_model.predict(X_val_select.values.astype(np.float32)))\n",
    "\n",
    "def comp_y_pred_nn2(p): ### this function fit a deep Neural network model and pred y value\n",
    "    select=X_train.columns[ df_topn[p] ]\n",
    "    X_val_select, X_train_select, y_val, y_train = X_val[select], X_train[select], Y_val[p], Y_train[p]\n",
    "    #'''fit model with selected features'''\n",
    "    my_model = NeuralNetRegressor( RegressorModule2(n_feature = topn ), max_epochs=10, lr=0.01, verbose=0, train_split=False)\n",
    "    \n",
    "    my_model.fit(X_train_select.values.astype(np.float32), y_train.values.astype(np.float32).reshape(-1,1))\n",
    "    return np.nan_to_num( my_model.predict(X_train_select.values.astype(np.float32))),                 np.nan_to_num( my_model.predict(X_val_select.values.astype(np.float32)))\n",
    "\n",
    "def comp_y_pred_nn1(p): ### this function fit a deep Neural network model and pred y value\n",
    "    select=X_train.columns[ df_topn[p] ]\n",
    "    X_val_select, X_train_select, y_val, y_train = X_val[select], X_train[select], Y_val[p], Y_train[p]\n",
    "    #'''fit model with selected features'''\n",
    "    my_model = NeuralNetRegressor( RegressorModule1(n_feature = topn ), max_epochs=10, lr=0.01, verbose=0, train_split=False)\n",
    "    \n",
    "    my_model.fit(X_train_select.values.astype(np.float32), y_train.values.astype(np.float32).reshape(-1,1))\n",
    "    return np.nan_to_num( my_model.predict(X_train_select.values.astype(np.float32))),                 np.nan_to_num( my_model.predict(X_val_select.values.astype(np.float32)))\n",
    "\n",
    "def comp_y_pred_baselineEN(p): ### this function fit a baseline Elastic Net model and pred y value\n",
    "    ## best lambda parameter, Constant that multiplies the penalty term\n",
    "    # baseline_EN, a ElasticNet model: The parameter l1_ratio corresponds to alpha in the glmnet R package,\n",
    "    # while alpha corresponds to the lambda parameter in glmnet\n",
    "    my_model= linear_model.ElasticNetCV(l1_ratio=0.5, cv = 5, fit_intercept=False, random_state=0, \n",
    "                                        alphas=np.arange(0.1, 0.9, 0.05), selection='random').fit(X_train, Y_train[p])# \n",
    "    return my_model.predict(X_train), my_model.predict(X_val)\n",
    "\n",
    "def comp_y_pred_teamHYU(p): \n",
    "# compare to RNA proxy. Random Forest based models (RF) were compared with a baseline method using 5-fold cross validation, in which the RNA level is directly used as a proxy of corresponding protein level. For all proteins RF delivered the best performance, or which do not have cor-\n",
    "# responding RNA data available, Random Forests were used. The baseline model was used for the remaining proteins\n",
    "    mask = final_feature[p] \n",
    "    if not mask.any(): ## no feature RNA passed threshold pearson > 0.3\n",
    "        if p in X_train.columns: # Special condition 1: mRNA proxy available, use it!\n",
    "            return X_train[p].values, X_val[p].values\n",
    "        else:  ## Special condition 2: no mRNA proxy available, return zeros\n",
    "            return np.zeros_like(Y_train[p]), np.zeros_like(Y_val[p])\n",
    "    \n",
    "    X_val_select, X_train_select = X_val.loc[:, mask ], X_train.loc[:, mask ]\n",
    "\n",
    "    if (p in both_RNA_protein):  ## protein and mRNA exists, RF vs Proxy, which better?\n",
    "        N=5\n",
    "        df_r_temp = pd.DataFrame(0, index= ['RF', 'Proxy'], columns = range(N))\n",
    "        i = 0\n",
    "        kf = KFold(n_splits=N, random_state=1, shuffle=True)\n",
    "        for train_index, test_index in kf.split(Y_train): \n",
    "            X_train_inner, X_val_inner = X_train_select.iloc[train_index],  X_train_select.iloc[test_index]\n",
    "            Y_train_inner, Y_val_inner = Y_train[p].iloc[train_index],  Y_train[p].iloc[test_index]\n",
    "            my_model = model_dict[m]\n",
    "            my_model.fit(X_train_inner, Y_train_inner)\n",
    "            Y_val_inner_pred = my_model.predict(X_val_inner)\n",
    "            # RF model performance\n",
    "            df_r_temp.loc['RF', i] =  np.corrcoef(Y_val_inner,  Y_val_inner_pred)[1,0] \n",
    "            # proxy model performance\n",
    "            df_r_temp.loc['Proxy', i] = np.corrcoef(Y_val_inner,  X_train[p][test_index] )[1,0] \n",
    "            i = i+1 \n",
    "        \n",
    "        if df_r_temp.mean(axis=1)['RF'] < df_r_temp.mean(axis=1)['Proxy']: # here, proxy wins\n",
    "            return X_train[p].values, X_val[p].values\n",
    "        \n",
    "    ## 'other cases, RF wins'\n",
    "    my_model = model_dict[m]\n",
    "    my_model.fit(X_train_select, Y_train[p])\n",
    "    return my_model.predict(X_train_select), my_model.predict(X_val_select)\n",
    "\n",
    "def sp_parallel(X, Y, proteins): ## comput spearman r block by block, more efficiently than whole X and Y\n",
    "    df_sp = pd.DataFrame(index= X.columns, columns=proteins)\n",
    "    for rnas in np.array_split(X.columns, n_set_x):\n",
    "        df_sp.loc[rnas, proteins] = pd.concat([ X.add_suffix('_x').loc[:,rnas +'_x'], Y.loc[:, proteins]], axis=1)        .corr(method='spearman').loc[rnas+'_x', proteins].values\n",
    "    return df_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run train and predict with 13 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cross_validation = True\n",
    "run_test = False\n",
    "list_model_done = [ ]\n",
    "print(time.ctime(), 'nCPU =', nCPU, 'run_cross_validation=', run_cross_validation, 'run_test=', run_test)\n",
    "\n",
    "########### 1. load and transform data \n",
    "X = X.transform(lambda x: (x-x.mean())/x.std(), axis=1).round(3).transpose()\n",
    "Y = Y.transform(lambda x: (x-x.mean())/x.std(), axis=1).round(3).transpose()\n",
    "\n",
    "my_list_topn = list_topn[::-1] \n",
    "print(time.ctime(), mykey, 'X',  X.shape, 'Y', Y.shape,  my_list_topn)\n",
    "\n",
    "df_template = pd.DataFrame(index= Y.columns, columns= range(N))\n",
    "res = create_res(my_list_topn)\n",
    "\n",
    "# number of blocks for spearman r computation, 200x100 unit\n",
    "n_set_x, n_set_y = np.floor_divide(X.shape[1], 200)+1, np.floor_divide(Y.shape[1], 100)+1\n",
    "\n",
    "########### 2. Pre-comput feature ranking by feature selection methods 1~4\n",
    "#rank RNAs by linear correlation, run once for the data set, use the matrix later, N-fold CV split i have its own feature ranking matrix\n",
    "dict_feature_rank = dict.fromkeys(method_feature_select[:-1])\n",
    "for mfs in dict_feature_rank:\n",
    "    dict_feature_rank[mfs] = {'CV': dict.fromkeys(range(N)), 'test': dict.fromkeys(range(N))}\n",
    "\n",
    "dict_peasonr_mat = dict.fromkeys(range(N))\n",
    "\n",
    "df_template = pd.DataFrame(index= Y.columns, columns= range(N))\n",
    "res = create_res(my_list_topn)\n",
    "\n",
    "## first, new machine learning models by me\n",
    "############## 1. rank all features by a method, run once for the data set, use the maxtrix later\n",
    "# split i have its own feature rankings\n",
    "if run_cross_validation == True:\n",
    "    print(time.ctime(), \"prepare training set.\")\n",
    "    # cross-validation on train set; \n",
    "    kf = KFold(n_splits=N, random_state=1, shuffle=True)\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(Y): \n",
    "        X_train, X_val = X.iloc[train_index],  X.iloc[test_index]\n",
    "        Y_train, Y_val = Y.iloc[train_index],  Y.iloc[test_index]\n",
    "        \n",
    "        cos = pd.DataFrame(data=metrics.pairwise.cosine_similarity(X=X_train.transpose(), Y=Y_train.transpose(), \n",
    "                                                                   dense_output=True), columns=Y.columns)\n",
    "        \n",
    "        with Pool(nCPU) as pool:\n",
    "            spearman_corr = pd.concat( pool.map( partial(sp_parallel, X_train, Y_train), np.array_split(Y.columns, n_set_y) ), axis=1)\n",
    "        \n",
    "        np.random.seed(i)\n",
    "        dict_feature_rank['random']['CV'][i] =             pd.DataFrame([ np.random.permutation(range(1,X.shape[1]+1)) for col in Y.columns], index = Y.columns).T\n",
    "        dict_feature_rank['cosine']['CV'][i] = abs(cos).rank(axis=0, ascending=False, method='first').astype(np.int32)\n",
    "        dict_feature_rank['raw_cosine']['CV'][i] = cos.rank(axis=0, ascending=False, method='first').astype(np.int32)\n",
    "        dict_feature_rank['spearmanr']['CV'][i] = spearman_corr.rank(axis=0, ascending=False, method='first').astype(np.int32)\n",
    "\n",
    "        dict_peasonr_mat[i] = pd.DataFrame( np.corrcoef(Y_train, X_train, rowvar=False)[Y.shape[1]:, :Y.shape[1]], \n",
    "                                       index=X.columns, columns=Y.columns) \n",
    "        i = i +1\n",
    "\n",
    "##### 2. cross-validaton model fit and evaluate with test\n",
    "########### 3. Classical machine learning models using prioritized features\n",
    "for mfs in method_feature_select[ :1]:\n",
    "    for topn in my_list_topn:\n",
    "        for m in list_model[ :-3]:\n",
    "            if (topn == X.shape[1] ) & (mfs !='cosine'): ### avoid recomputing all features, \n",
    "                res[mfs][topn][m] = deepcopy( res['cosine'][topn][m])\n",
    "                continue\n",
    "            print(time.ctime(), mfs.ljust(8,'-'), str(topn).ljust(8,'-'), m.ljust(10,'-'), end=\":  \")\n",
    "            \n",
    "            if run_cross_validation: # cross-validaton, model fit and evaluate with test\n",
    "                start =time.time()\n",
    "                print(\"training split \", end=\" \")\n",
    "                i = 0\n",
    "                for train_index, test_index in kf.split(Y): # random trials to stablize errors\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "                    Y_train, Y_val = Y.iloc[train_index], Y.iloc[test_index]\n",
    "                    \n",
    "                    df_topn = ( dict_feature_rank[mfs]['CV'][i] <= topn ) ## feature selection\n",
    "\n",
    "                    if m.startswith('NN'): # faster to use forloop\n",
    "                        if m == 'NN1':\n",
    "                            comp_y_pred_nn = comp_y_pred_nn1\n",
    "                        elif m == 'NN2':\n",
    "                            comp_y_pred_nn = comp_y_pred_nn2\n",
    "                        elif m == 'NN3':\n",
    "                            comp_y_pred_nn = comp_y_pred_nn3\n",
    "                        else:\n",
    "                            print(m)\n",
    "                            raise NameError \n",
    "                        y_pred_train_in_rows, y_pred_val_in_rows = list(),list()  \n",
    "                        for g in Y.columns:\n",
    "                            x1, x2 = comp_y_pred_nn(g)\n",
    "                            y_pred_train_in_rows.extend( x1.reshape(1,-1))\n",
    "                            y_pred_val_in_rows.extend( x2.reshape(1,-1))\n",
    "\n",
    "                    else:\n",
    "                        with Pool(nCPU) as pool:\n",
    "                            y_pred_in_rows = []\n",
    "                            temp = pool.map_async(comp_y_pred, Y_train.columns,  callback=y_pred_in_rows.extend)\n",
    "                            temp.wait()\n",
    "                            y_pred_train_in_rows, y_pred_val_in_rows = zip(*y_pred_in_rows)\n",
    "\n",
    "                    ## store y_pred\n",
    "                    res[mfs][topn][m][\"y_pred_CVtest\"][i] = y_pred_val_in_rows\n",
    "                    res[mfs][topn][m][\"y_pred_CVtrain\"][i] = y_pred_train_in_rows\n",
    "                    \n",
    "                    i = i + 1;  print(i, end=\" \");  # end for split i\n",
    "                # end:  N fold\n",
    "                res[mfs][topn][\"time\"].loc[m, \"val\"] =  int(time.time() - start)\n",
    "\n",
    "            list_model_done.extend([ m ])\n",
    "            print(\"done.\")\n",
    "\n",
    "list_model_done = [m for m in list_model if m in list_model_done]\n",
    "print(time.ctime(), \"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predict result. Later can do performance measure, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    outfile = '/public/home/test1/mydata/proteome/data/res_v4.3_' +time.strftime(\"%Y%m%d-%H%M\")+    '_'+'-'.join(list_model_done)+'_'+ mykey + '.pkl'\n",
    "    with open(outfile, 'wb') as f: \n",
    "        pickle.dump( [res],  f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.7.3_skorch]",
   "language": "python",
   "name": "conda-env-py3.7.3_skorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
