{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import time, os, sys\n",
    "\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor, VotingRegressor, AdaBoostRegressor, BaggingRegressor\n",
    "\n",
    "import pickle\n",
    "from scipy import stats \n",
    "\n",
    "# \n",
    "import mkl\n",
    "mkl.set_num_threads(1)\n",
    "nCPU= 30\n",
    "N=5\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.4\n"
     ]
    }
   ],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/public/home/test1/mydata/proteome/data/20210414_dict_matrix_17dataset.pkl'\n",
    "file = '/media/eys/xwj/proteome/data/20211022_dict_matrix_20dataset.pkl'\n",
    "with open(file, 'rb') as f:\n",
    "    [ dict_dataset, df_summary]=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction of RNA to protein in brain tissue\n",
    "\n",
    "file = '/public/home/test1/mydata/proteome/data/RNA_before_pred.pkl'\n",
    "with open(file, 'rb') as f:\n",
    "    [ brain_atlas_RNA ]=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14729, 231)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_atlas_RNA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14408, 71) (14729, 231) 13367\n",
      "brain_71_2017_labelfree (71, 13367) (71, 4385) (231, 13367) (71, 13367) (71, 4385)\n",
      "[10, 20, 50, 100, 200, 500, 1000, 5000, 13367]\n"
     ]
    }
   ],
   "source": [
    "## brain X and Y , because only some genes are common between training and test datasets\n",
    "## overlapping genes~ 13000/14000\n",
    "common = dict_dataset[mykey][\"RNA\"].index.intersection( brain_atlas_RNA.index )\n",
    "\n",
    "print(dict_dataset[mykey][\"RNA\"].shape, brain_atlas_RNA.shape, len(common))\n",
    "\n",
    "X = dict_dataset[mykey][\"RNA\"].loc[common].transform(lambda x: (x-x.mean())/x.std(), axis=1).round(3).transpose()\n",
    "Y = dict_dataset[mykey][\"protein\"].transform(lambda x: (x-x.mean())/x.std(), axis=1).round(3).transpose()\n",
    "\n",
    "## brain atlas\n",
    "X_test = brain_atlas_RNA.loc[common].transform(lambda x: (x-x.mean())/x.std(), axis=1).round(3).transpose()\n",
    "\n",
    "X_use, Y_use = X, Y\n",
    "print(mykey, X.shape, Y.shape, X_test.shape,  X_use.shape,   Y_use.shape)\n",
    "\n",
    "my_list_topn = list_topn + [X.shape[1]]\n",
    "print(my_list_topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brain_71_2017_labelfree (71, 13367) (71, 4385) (231, 13367) (231, 4385) (71, 13367) (71, 4385)\n"
     ]
    }
   ],
   "source": [
    "## make an empty Y_test, the pipeline must need ONE\n",
    "Y_test = pd.DataFrame( index = X_test.index, columns= Y.columns)\n",
    "print(mykey, X.shape, Y.shape, X_test.shape, Y_test.shape, X_use.shape,   Y_use.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data distribution is ok\n",
    "for i in range(100,110,1):\n",
    "    X_test.iloc[:, i].hist(bins=20, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/media/eys/xwj/proteome/data/df_metric_list_topn_20220104.pkl'\n",
    "with open(file, 'rb') as f:\n",
    "    [ df_metric_list_topn, df_metric_list_topn2 ]=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/home/test1/soft/anaconda3/envs/py3.7.3_skorch/lib/python3.7/site-packages/pandas/core/indexing.py:925: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 'All', 'All')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mykey='brain_71_2017_labelfree'\n",
    "df_metric_list_topn.loc[ (mykey,'Voting'), ('cosine', 'r')].idxmax(), \\\n",
    "df_metric_list_topn.loc[ (mykey,'Voting'), ('cosine', 'rmse')].idxmin(), \\\n",
    "df_metric_list_topn.loc[ (mykey,'Voting'), ('cosine', 'mae')].idxmin(), \\\n",
    "## best topn is 5000,  use 1000 gene should be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LR': LinearRegression(), 'Lasso': Lasso(alpha=0.02, max_iter=100000.0), 'HR': HuberRegressor(), 'Ridge': Ridge(), 'SVR': SVR(), 'RFR': RandomForestRegressor(max_depth=3, random_state=0), 'NN1': None, 'NN2': None, 'NN3': None, 'Stacking': StackingRegressor(estimators=[('LR', LinearRegression()),\n",
      "                              ('Lasso', Lasso(alpha=0.02, max_iter=100000.0)),\n",
      "                              ('HR', HuberRegressor()), ('Ridge', Ridge()),\n",
      "                              ('SVR', SVR()),\n",
      "                              ('RFR',\n",
      "                               RandomForestRegressor(max_depth=3,\n",
      "                                                     random_state=0))]), 'Voting': VotingRegressor(estimators=[('LR', LinearRegression()),\n",
      "                            ('Lasso', Lasso(alpha=0.02, max_iter=100000.0)),\n",
      "                            ('HR', HuberRegressor()), ('Ridge', Ridge()),\n",
      "                            ('SVR', SVR()),\n",
      "                            ('RFR',\n",
      "                             RandomForestRegressor(max_depth=3,\n",
      "                                                   random_state=0))]), 'Boosting': AdaBoostRegressor(random_state=0), 'Bagging': BaggingRegressor(random_state=0), 'baselineEN': ElasticNet(precompute=True, random_state=0), 'teamHYU': RandomForestRegressor(random_state=0), 'teamHL&YG': None} ['LR', 'Lasso', 'HR', 'Ridge', 'SVR', 'RFR', 'NN1', 'NN2', 'NN3', 'Stacking', 'Voting', 'Boosting', 'Bagging', 'baselineEN', 'teamHYU', 'teamHL&YG']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_dict = {\n",
    "    \"LR\": linear_model.LinearRegression(), \n",
    "    \"Lasso\": linear_model.Lasso(alpha=0.02, max_iter=1e5), \n",
    "    \"HR\": linear_model.HuberRegressor(), #Linear regression model that is robust to outliers.\n",
    "    \"Ridge\": linear_model.Ridge(), \n",
    "    \"SVR\": SVR( gamma='scale'),\n",
    "    \"RFR\": RandomForestRegressor(n_estimators=100, max_depth=3, random_state=0), \n",
    "    # NN : construct later, because feature length have to be set \n",
    "    \"NN1\": None,\n",
    "    \"NN2\": None,\n",
    "    \"NN3\": None,\n",
    "    ##### other ensemble models\n",
    "    \"Stacking\": None,\n",
    "    \"Voting\": None,\n",
    "    \"Boosting\": None,\n",
    "    \"Bagging\": None, \n",
    "    ##### reimplement published methods\n",
    "    \"baselineEN\": linear_model.ElasticNet(l1_ratio = 0.5, random_state = 0, precompute=True), \n",
    "    \"teamHYU\": RandomForestRegressor(n_estimators=100, random_state=0), # Author not providing detail, use default\n",
    "    \"teamHL&YG\": None, ## equivalent to RFR without feature selection\n",
    "    }\n",
    "\n",
    "basic_estimators = [\n",
    "    (\"LR\",  model_dict['LR']), \n",
    "    (\"Lasso\", model_dict['Lasso']), \n",
    "    (\"HR\",  model_dict['HR']),\n",
    "    (\"Ridge\", model_dict['Ridge']), \n",
    "    (\"SVR\", model_dict['SVR']),\n",
    "    (\"RFR\", model_dict['RFR']), \n",
    "    ]\n",
    " \n",
    "model_dict['Stacking'] = StackingRegressor(estimators=basic_estimators)\n",
    "model_dict['Voting'] = VotingRegressor(basic_estimators)\n",
    "model_dict['Boosting']  = AdaBoostRegressor(random_state=0)\n",
    "model_dict['Bagging'] = BaggingRegressor(random_state=0)\n",
    "    \n",
    "list_model = list(model_dict.keys())\n",
    "print( model_dict, list_model )\n",
    "method_feature_select = [\"cosine\", \"raw_cosine\", \"spearmanr\", \"random\", \"custom\"]\n",
    "\n",
    "# feature numbers to be prioritized in feature selection \n",
    "list_topn = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 200, 1000, 5000 ] # v1\n",
    "list_topn = [5, 10, 20, 30, 40, 50, 100, 200, 500, 1000, 5000 ] # v2\n",
    "list_topn = [10, 20, 50, 100, 200, 500, 1000, 5000  ] # v3\n",
    "\n",
    "#############\n",
    "def create_res(list_topn):\n",
    "    res = {}\n",
    "    for mfs in method_feature_select:\n",
    "        res[mfs] = {} \n",
    "        for topn in list_topn:\n",
    "            # summary table\n",
    "            res[mfs][topn] = {}\n",
    "            res[mfs][topn][\"time\"] = pd.DataFrame(data = 0, dtype = np.int32, index = model_dict, columns= [\"val\",\"train\",\"test\"])\n",
    "            # detail data for all models\n",
    "            for m in model_dict:\n",
    "                res[mfs][topn][m]={}\n",
    "                res[mfs][topn][m][\"y_pred_CVtrain\"], res[mfs][topn][m][\"y_pred_CVtest\"] ={}, {}\n",
    "    return res\n",
    "\n",
    "# comp_y_pred only output y_predict value\n",
    "def comp_y_pred(p): ### this function fit a classical machine learning model and pred y value\n",
    "    # usage: comp_y_pred(\"ENSG00000000419\") \n",
    "    select=X_train.columns[ df_topn[p] ]\n",
    "    X_val_select, X_train_select, y_val, y_train = X_val[select], X_train[select], Y_val[p], Y_train[p]\n",
    "    my_model=model_dict[m].fit(X_train_select, y_train) #'''fit model with selected features'''\n",
    "    # training fit and test fit\n",
    "    return my_model.predict(X_train_select), my_model.predict(X_val_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  7 21:38:56 2022 nCPU = 30 run_cross_validation= False run_test= True\n",
      "/public/home/test1/mydata/proteome/data/res_v4.3_20220107-2138_temp_brain_71_2017_labelfree.pkl\n",
      "Fri Jan  7 21:38:56 2022 brain_71_2017_labelfree X (71, 13367) Y (71, 4385) [13367, 5000, 1000, 500, 200, 100, 50, 20, 10]\n",
      "Fri Jan  7 21:38:56 2022 prepare test set.\n",
      "Fri Jan  7 21:39:02 2022 cosine-- 1000---- Voting----:  30\n",
      "done.\n",
      "Fri Jan  7 21:42:12 2022 cosine-- 5000---- Voting----:  30\n",
      "done.\n",
      "Fri Jan  7 21:56:02 2022 complete /public/home/test1/mydata/proteome/data/res_v4.3_20220107-2156_Voting_brain_71_2017_labelfree.pkl\n"
     ]
    }
   ],
   "source": [
    "run_cross_validation = False\n",
    "run_test = True\n",
    "list_model_done = [ ]\n",
    "print(time.ctime(), 'nCPU =', nCPU, 'run_cross_validation=', run_cross_validation, 'run_test=', run_test)\n",
    "outfile = '/public/home/test1/mydata/proteome/data/res_v4.3_' +time.strftime(\"%Y%m%d-%H%M\")+\\\n",
    "    '_temp_'+ mykey + '.pkl'\n",
    "print(outfile)\n",
    "########### 1. load and transform data \n",
    "\n",
    "my_list_topn = [X.shape[1]] + list_topn[::-1] \n",
    "print(time.ctime(), mykey, 'X',  X.shape, 'Y', Y.shape,  my_list_topn)\n",
    "\n",
    "df_template = pd.DataFrame(index= Y.columns, columns= range(N))\n",
    "res = create_res(my_list_topn)\n",
    "\n",
    "########### 2. Pre-comput feature ranking by feature selection methods 1~4\n",
    "#rank RNAs by linear correlation, run once for the data set, use the matrix later, N-fold CV split i have its own feature ranking matrix\n",
    "dict_feature_rank = dict.fromkeys(method_feature_select[:-1])\n",
    "for mfs in dict_feature_rank:\n",
    "    dict_feature_rank[mfs] = {'CV': dict.fromkeys(range(N)), 'test': dict.fromkeys(range(N))}\n",
    "\n",
    "df_template = pd.DataFrame(index= Y.columns, columns= range(N))\n",
    "res = create_res(my_list_topn)\n",
    "\n",
    "## first, new machine learning models by me\n",
    "############## 1. rank all features by a method, run once for the data set, use the maxtrix later\n",
    "if run_test == True:\n",
    "    print(time.ctime(), \"prepare test set.\")\n",
    "    cos = pd.DataFrame(data=metrics.pairwise.cosine_similarity(X=X.transpose(), Y=Y.transpose(), \n",
    "                                                           dense_output=True), columns=Y.columns)\n",
    "    dict_feature_rank['cosine']['test'] = abs(cos).rank(axis=0, ascending=False, method='first').astype(np.int32)\n",
    "    \n",
    "##### 2. cross-validaton model fit and evaluate with test\n",
    "########### 3. Classical machine learning models using prioritized features\n",
    "for mfs in ['cosine']: #method_feature_select[ :-1]:\n",
    "    for topn in [1000, 5000]: #my_list_topn:\n",
    "        for m in ['Voting']: #list_model[ :-3]:\n",
    "            if (topn == X.shape[1] ) & (mfs !='cosine'): ### avoid recomputing all features, \n",
    "                res[mfs][topn][m] = deepcopy( res['cosine'][topn][m])\n",
    "                continue\n",
    "            print(time.ctime(), mfs.ljust(8,'-'), str(topn).ljust(8,'-'), m.ljust(10,'-'), end=\":  \")\n",
    "            \n",
    "            if run_test:\n",
    "                start =time.time()\n",
    "                X_train, X_val = X, X_test\n",
    "                Y_train, Y_val = Y, Y_test\n",
    "\n",
    "                df_topn = ( dict_feature_rank[mfs]['test'] <= topn ) ## feature selection\n",
    "                \n",
    "                with Pool(nCPU) as pool:\n",
    "                    print(nCPU)\n",
    "                    y_pred_in_rows = []\n",
    "                    temp = pool.map_async(comp_y_pred, Y_train.columns,  callback=y_pred_in_rows.extend)\n",
    "                    temp.wait()\n",
    "                    y_pred_train_in_rows, y_pred_val_in_rows = zip(*y_pred_in_rows)\n",
    "                ## store y_pred\n",
    "                res[mfs][topn][m][\"y_pred_test\"] = y_pred_val_in_rows\n",
    "                res[mfs][topn][m][\"y_pred_train\"] = y_pred_train_in_rows\n",
    "                res[mfs][topn][\"time\"].loc[m, \"test\"] =  int(time.time() - start) \n",
    "                \n",
    "            list_model_done.extend([ m ])\n",
    "            with open(outfile, 'wb') as f: \n",
    "                pickle.dump( [res],  f)\n",
    "            print(\"done.\")\n",
    "            \n",
    "list_model_done = [m for m in list_model if m in list_model_done]\n",
    "outfile = '/public/home/test1/mydata/proteome/data/res_v4.3_' +time.strftime(\"%Y%m%d-%H%M\")+    '_'+'-'.join(list_model_done)+'_'+ mykey + '.pkl'\n",
    "with open(outfile, 'wb') as f: \n",
    "    pickle.dump( [res],  f)\n",
    "print(time.ctime(), \"complete\", outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = pd.DataFrame(res[mfs][topn][m][\"y_pred_test\"], index= Y_train.columns, columns=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/media/eys/xwj/proteome/data/df_protein_predicted_brainatlas\"+ mykey+ \"%s.xlsx\" % time.strftime(\"%Y%m%d\")\n",
    "with pd.ExcelWriter(file, mode='w') as writer: \n",
    "    ## original RNA profiles\n",
    "    brain_atlas_RNA.to_excel(writer, sheet_name='brain_atlas_RNA' )\n",
    "    ### predicted protein profile result\n",
    "    y_pred_test.to_excel(writer, sheet_name='pred_by_'+mykey )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine 5000 Voting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sample_name</th>\n",
       "      <th>S010002</th>\n",
       "      <th>S010003</th>\n",
       "      <th>S010006</th>\n",
       "      <th>S010007</th>\n",
       "      <th>S010017</th>\n",
       "      <th>S010021</th>\n",
       "      <th>S010034</th>\n",
       "      <th>S010035</th>\n",
       "      <th>S010044</th>\n",
       "      <th>S010046</th>\n",
       "      <th>...</th>\n",
       "      <th>S020448</th>\n",
       "      <th>S020492</th>\n",
       "      <th>S020495</th>\n",
       "      <th>S020555</th>\n",
       "      <th>S020560</th>\n",
       "      <th>S020562</th>\n",
       "      <th>S020656</th>\n",
       "      <th>S020671</th>\n",
       "      <th>S020697</th>\n",
       "      <th>S020722</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000000419</th>\n",
       "      <td>-0.065661</td>\n",
       "      <td>0.121245</td>\n",
       "      <td>0.072144</td>\n",
       "      <td>-0.261557</td>\n",
       "      <td>0.396354</td>\n",
       "      <td>-0.231029</td>\n",
       "      <td>0.417294</td>\n",
       "      <td>0.464590</td>\n",
       "      <td>-0.181127</td>\n",
       "      <td>0.179913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>-0.078221</td>\n",
       "      <td>0.440084</td>\n",
       "      <td>-0.015687</td>\n",
       "      <td>0.350751</td>\n",
       "      <td>-0.129746</td>\n",
       "      <td>-0.833556</td>\n",
       "      <td>-1.056170</td>\n",
       "      <td>-0.842833</td>\n",
       "      <td>-0.957920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000000971</th>\n",
       "      <td>0.045746</td>\n",
       "      <td>-0.284783</td>\n",
       "      <td>-0.094662</td>\n",
       "      <td>0.040904</td>\n",
       "      <td>0.099710</td>\n",
       "      <td>-0.076298</td>\n",
       "      <td>-0.592523</td>\n",
       "      <td>-0.171447</td>\n",
       "      <td>0.062105</td>\n",
       "      <td>-0.142362</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.566994</td>\n",
       "      <td>-0.890966</td>\n",
       "      <td>-0.755587</td>\n",
       "      <td>0.262228</td>\n",
       "      <td>-0.378809</td>\n",
       "      <td>-0.861979</td>\n",
       "      <td>0.920954</td>\n",
       "      <td>0.982397</td>\n",
       "      <td>0.991599</td>\n",
       "      <td>0.780500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000001084</th>\n",
       "      <td>0.260922</td>\n",
       "      <td>0.163857</td>\n",
       "      <td>-0.129365</td>\n",
       "      <td>-0.320284</td>\n",
       "      <td>0.031953</td>\n",
       "      <td>0.500173</td>\n",
       "      <td>-0.272144</td>\n",
       "      <td>-0.221043</td>\n",
       "      <td>0.193509</td>\n",
       "      <td>-0.354602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008216</td>\n",
       "      <td>-0.448634</td>\n",
       "      <td>-0.528865</td>\n",
       "      <td>1.149949</td>\n",
       "      <td>1.058553</td>\n",
       "      <td>-0.648198</td>\n",
       "      <td>-1.788100</td>\n",
       "      <td>-1.378046</td>\n",
       "      <td>-1.150940</td>\n",
       "      <td>-1.444326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000001561</th>\n",
       "      <td>0.650607</td>\n",
       "      <td>0.161589</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>1.035660</td>\n",
       "      <td>0.158322</td>\n",
       "      <td>0.439346</td>\n",
       "      <td>-0.237918</td>\n",
       "      <td>-0.038107</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>-0.037303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.624667</td>\n",
       "      <td>-0.737336</td>\n",
       "      <td>-0.931410</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>-0.192389</td>\n",
       "      <td>-0.931205</td>\n",
       "      <td>-2.263456</td>\n",
       "      <td>-2.229748</td>\n",
       "      <td>-2.057440</td>\n",
       "      <td>-2.154113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000001629</th>\n",
       "      <td>0.264622</td>\n",
       "      <td>0.428375</td>\n",
       "      <td>0.227309</td>\n",
       "      <td>0.155297</td>\n",
       "      <td>0.688891</td>\n",
       "      <td>0.600820</td>\n",
       "      <td>0.880902</td>\n",
       "      <td>0.802693</td>\n",
       "      <td>0.636837</td>\n",
       "      <td>0.582517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280222</td>\n",
       "      <td>0.276196</td>\n",
       "      <td>-0.158796</td>\n",
       "      <td>-1.432054</td>\n",
       "      <td>-0.759790</td>\n",
       "      <td>0.237514</td>\n",
       "      <td>-0.555512</td>\n",
       "      <td>-0.471963</td>\n",
       "      <td>-0.572709</td>\n",
       "      <td>-0.531192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000277363</th>\n",
       "      <td>0.484906</td>\n",
       "      <td>0.113828</td>\n",
       "      <td>0.190436</td>\n",
       "      <td>0.336553</td>\n",
       "      <td>0.192715</td>\n",
       "      <td>0.555451</td>\n",
       "      <td>0.262029</td>\n",
       "      <td>0.265669</td>\n",
       "      <td>0.312289</td>\n",
       "      <td>0.210727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198949</td>\n",
       "      <td>-0.477678</td>\n",
       "      <td>-0.808870</td>\n",
       "      <td>-0.070302</td>\n",
       "      <td>-0.580827</td>\n",
       "      <td>-0.536488</td>\n",
       "      <td>0.700610</td>\n",
       "      <td>0.499506</td>\n",
       "      <td>0.749692</td>\n",
       "      <td>0.669856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000277443</th>\n",
       "      <td>0.021876</td>\n",
       "      <td>-0.202222</td>\n",
       "      <td>-0.043727</td>\n",
       "      <td>0.264475</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>-0.143440</td>\n",
       "      <td>0.070518</td>\n",
       "      <td>-0.161789</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.186490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.402964</td>\n",
       "      <td>-0.132596</td>\n",
       "      <td>-0.131425</td>\n",
       "      <td>-0.554127</td>\n",
       "      <td>-1.096666</td>\n",
       "      <td>-0.031763</td>\n",
       "      <td>2.616940</td>\n",
       "      <td>2.340987</td>\n",
       "      <td>2.554846</td>\n",
       "      <td>2.547615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000277586</th>\n",
       "      <td>0.678800</td>\n",
       "      <td>0.495665</td>\n",
       "      <td>-0.143843</td>\n",
       "      <td>0.076956</td>\n",
       "      <td>-0.270272</td>\n",
       "      <td>0.321970</td>\n",
       "      <td>-0.698347</td>\n",
       "      <td>-0.852038</td>\n",
       "      <td>-0.160584</td>\n",
       "      <td>-0.465839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038311</td>\n",
       "      <td>-0.370473</td>\n",
       "      <td>-0.607690</td>\n",
       "      <td>1.544494</td>\n",
       "      <td>0.898343</td>\n",
       "      <td>-0.525598</td>\n",
       "      <td>-1.659769</td>\n",
       "      <td>-1.349344</td>\n",
       "      <td>-1.336161</td>\n",
       "      <td>-1.692258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000277791</th>\n",
       "      <td>0.041465</td>\n",
       "      <td>-0.363506</td>\n",
       "      <td>0.573067</td>\n",
       "      <td>0.407246</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>-0.384949</td>\n",
       "      <td>0.363280</td>\n",
       "      <td>0.532473</td>\n",
       "      <td>-0.295805</td>\n",
       "      <td>0.458752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.403503</td>\n",
       "      <td>-0.421783</td>\n",
       "      <td>-0.904271</td>\n",
       "      <td>-1.277726</td>\n",
       "      <td>-1.238096</td>\n",
       "      <td>0.043569</td>\n",
       "      <td>1.331203</td>\n",
       "      <td>0.485969</td>\n",
       "      <td>0.983241</td>\n",
       "      <td>0.992514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000279457</th>\n",
       "      <td>-0.340045</td>\n",
       "      <td>-0.703444</td>\n",
       "      <td>-0.119692</td>\n",
       "      <td>0.110721</td>\n",
       "      <td>-0.214701</td>\n",
       "      <td>-0.740557</td>\n",
       "      <td>-0.634201</td>\n",
       "      <td>-0.801116</td>\n",
       "      <td>-0.219594</td>\n",
       "      <td>-0.470434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158947</td>\n",
       "      <td>0.149605</td>\n",
       "      <td>-0.079439</td>\n",
       "      <td>1.032536</td>\n",
       "      <td>0.116007</td>\n",
       "      <td>0.352293</td>\n",
       "      <td>0.216731</td>\n",
       "      <td>0.318026</td>\n",
       "      <td>0.235725</td>\n",
       "      <td>0.162772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4385 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "sample_name       S010002   S010003   S010006   S010007   S010017   S010021  \\\n",
       "ENSG00000000419 -0.065661  0.121245  0.072144 -0.261557  0.396354 -0.231029   \n",
       "ENSG00000000971  0.045746 -0.284783 -0.094662  0.040904  0.099710 -0.076298   \n",
       "ENSG00000001084  0.260922  0.163857 -0.129365 -0.320284  0.031953  0.500173   \n",
       "ENSG00000001561  0.650607  0.161589  0.817700  1.035660  0.158322  0.439346   \n",
       "ENSG00000001629  0.264622  0.428375  0.227309  0.155297  0.688891  0.600820   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000277363  0.484906  0.113828  0.190436  0.336553  0.192715  0.555451   \n",
       "ENSG00000277443  0.021876 -0.202222 -0.043727  0.264475  0.235200 -0.143440   \n",
       "ENSG00000277586  0.678800  0.495665 -0.143843  0.076956 -0.270272  0.321970   \n",
       "ENSG00000277791  0.041465 -0.363506  0.573067  0.407246  0.412800 -0.384949   \n",
       "ENSG00000279457 -0.340045 -0.703444 -0.119692  0.110721 -0.214701 -0.740557   \n",
       "\n",
       "sample_name       S010034   S010035   S010044   S010046  ...   S020448  \\\n",
       "ENSG00000000419  0.417294  0.464590 -0.181127  0.179913  ... -0.024240   \n",
       "ENSG00000000971 -0.592523 -0.171447  0.062105 -0.142362  ... -0.566994   \n",
       "ENSG00000001084 -0.272144 -0.221043  0.193509 -0.354602  ...  0.008216   \n",
       "ENSG00000001561 -0.237918 -0.038107  0.026042 -0.037303  ... -0.624667   \n",
       "ENSG00000001629  0.880902  0.802693  0.636837  0.582517  ... -0.280222   \n",
       "...                   ...       ...       ...       ...  ...       ...   \n",
       "ENSG00000277363  0.262029  0.265669  0.312289  0.210727  ... -0.198949   \n",
       "ENSG00000277443  0.070518 -0.161789  0.002162  0.186490  ... -0.402964   \n",
       "ENSG00000277586 -0.698347 -0.852038 -0.160584 -0.465839  ...  0.038311   \n",
       "ENSG00000277791  0.363280  0.532473 -0.295805  0.458752  ... -0.403503   \n",
       "ENSG00000279457 -0.634201 -0.801116 -0.219594 -0.470434  ... -0.158947   \n",
       "\n",
       "sample_name       S020492   S020495   S020555   S020560   S020562   S020656  \\\n",
       "ENSG00000000419 -0.078221  0.440084 -0.015687  0.350751 -0.129746 -0.833556   \n",
       "ENSG00000000971 -0.890966 -0.755587  0.262228 -0.378809 -0.861979  0.920954   \n",
       "ENSG00000001084 -0.448634 -0.528865  1.149949  1.058553 -0.648198 -1.788100   \n",
       "ENSG00000001561 -0.737336 -0.931410  0.018884 -0.192389 -0.931205 -2.263456   \n",
       "ENSG00000001629  0.276196 -0.158796 -1.432054 -0.759790  0.237514 -0.555512   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "ENSG00000277363 -0.477678 -0.808870 -0.070302 -0.580827 -0.536488  0.700610   \n",
       "ENSG00000277443 -0.132596 -0.131425 -0.554127 -1.096666 -0.031763  2.616940   \n",
       "ENSG00000277586 -0.370473 -0.607690  1.544494  0.898343 -0.525598 -1.659769   \n",
       "ENSG00000277791 -0.421783 -0.904271 -1.277726 -1.238096  0.043569  1.331203   \n",
       "ENSG00000279457  0.149605 -0.079439  1.032536  0.116007  0.352293  0.216731   \n",
       "\n",
       "sample_name       S020671   S020697   S020722  \n",
       "ENSG00000000419 -1.056170 -0.842833 -0.957920  \n",
       "ENSG00000000971  0.982397  0.991599  0.780500  \n",
       "ENSG00000001084 -1.378046 -1.150940 -1.444326  \n",
       "ENSG00000001561 -2.229748 -2.057440 -2.154113  \n",
       "ENSG00000001629 -0.471963 -0.572709 -0.531192  \n",
       "...                   ...       ...       ...  \n",
       "ENSG00000277363  0.499506  0.749692  0.669856  \n",
       "ENSG00000277443  2.340987  2.554846  2.547615  \n",
       "ENSG00000277586 -1.349344 -1.336161 -1.692258  \n",
       "ENSG00000277791  0.485969  0.983241  0.992514  \n",
       "ENSG00000279457  0.318026  0.235725  0.162772  \n",
       "\n",
       "[4385 rows x 231 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mfs, topn, m)\n",
    "y_pred_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.7.3_skorch]",
   "language": "python",
   "name": "conda-env-py3.7.3_skorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
